# ---
# jupyter:
#   jupytext:
#     text_representation:
#       extension: .py
#       format_name: percent
#       format_version: '1.3'
#       jupytext_version: 1.14.5
#   kernelspec:
#     display_name: Python 3
#     language: python
#     name: python3
# ---
# -*- coding: utf-8 -*-
"""
Experiment Report
Generated automatically from experiment data
Experiment Path: {{ experiment_path }}
"""

# %% [markdown]
# # Experiment Report
#
# **Experiment Path:** `{{ experiment_path }}`

# %%
# Imports
import base64
import json
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from pathlib import Path
from hercule.environnements import EnvironmentInspector, load_environment

# %% [markdown]
# ## Experiment Overview

# %%
# Get the directory where this report is located (same as experiment directory)
experiment_dir = Path(__file__).parent.resolve()

# Load environment configuration
with open(experiment_dir / "{{ environment_file_name }}", encoding="utf-8") as f:
    environment_config = json.load(f)

# %% [markdown]
# ### Environment Configuration
#
# Environment configuration used for the experiment:

# %%
print("## Environment Configuration")
print("```json")
print(json.dumps(environment_config, indent=2))
print("```")

# %% [markdown]
# ### Environment Details
#
# Detailed information about the environment's observation and action spaces:

# %%
# Load environment to get detailed information
env = load_environment(experiment_dir / "{{ environment_file_name }}")
env_info = EnvironmentInspector.get_environment_info(env)
env.close()

# Display observation space details
print("### Observation Space")
obs_space = env_info.observation_space
print(f"- **Type:** {obs_space.type}")
if obs_space.type == "Discrete":
    print(f"- **Space Type:** Discrete")
    print(f"- **Number of possible observations:** {obs_space.n}")
    print(f"- **Shape:** N/A (discrete space)")
elif obs_space.type == "Box":
    print(f"- **Space Type:** Continuous (Box)")
    print(f"- **Shape:** {obs_space.shape}")
    if obs_space.low is not None and obs_space.high is not None:
        print(f"- **Lower bounds:** {obs_space.low}")
        print(f"- **Upper bounds:** {obs_space.high}")
else:
    print(f"- **Shape:** {obs_space.shape}")

# Display action space details
print("\n### Action Space")
action_space = env_info.action_space
print(f"- **Type:** {action_space.type}")
if action_space.type == "Discrete":
    print(f"- **Space Type:** Discrete")
    print(f"- **Number of actions:** {action_space.n}")
    print(f"- **Shape:** N/A (discrete space)")
    print(f"- **Possible actions:** {list(range(action_space.n))}")
elif action_space.type == "Box":
    print(f"- **Space Type:** Continuous (Box)")
    print(f"- **Shape:** {action_space.shape}")
    if action_space.low is not None and action_space.high is not None:
        print(f"- **Lower bounds:** {action_space.low}")
        print(f"- **Upper bounds:** {action_space.high}")
        print(f"- **Action range:** Each action dimension is bounded by the above limits")
else:
    print(f"- **Shape:** {action_space.shape}")

# Display reward information if available
print("\n### Reward Information")
if env_info.spec_info and env_info.spec_info.reward_threshold is not None:
    print(f"- **Reward threshold:** {env_info.spec_info.reward_threshold}")
    print("  (This is the reward threshold considered for solving the environment)")
else:
    print("- **Reward threshold:** Not specified in environment spec")

# %%
# Load model configuration
with open(experiment_dir / "{{ model_file_name }}", encoding="utf-8") as f:
    model_config = json.load(f)

# %% [markdown]
# ### Model Configuration
#
# Model configuration used for the experiment:

# %%
print("## Model Configuration")
print("```json")
print(json.dumps(model_config, indent=2))
print("```")

# %%
# Load run_info data
with open(experiment_dir / "{{ run_info_file_name }}", encoding="utf-8") as f:
    run_info_data = json.load(f)

# %% [markdown]
# ### Training Information
#
# Training information from the experiment:

# %%
print("### Training Information")
print(f"- **Learning Epochs:** {run_info_data.get('learning_ongoing_epoch', 0)}")
print(f"- **Testing Epochs:** {run_info_data.get('testing_ongoing_epoch', 0)}")

# %% [markdown]
# ## Data Loading and Preparation

# %%
# Load experiment data from run_info
learning_rewards = [metric.get("reward", 0) for metric in run_info_data.get("learning_metrics", [])]
learning_steps = [metric.get("steps_number", 0) for metric in run_info_data.get("learning_metrics", [])]
testing_rewards = [metric.get("reward", 0) for metric in run_info_data.get("testing_metrics", [])]
testing_steps = [metric.get("steps_number", 0) for metric in run_info_data.get("testing_metrics", [])]

# %% [markdown]
# ### Data Summary

# %%
# Display data summary
print("## Data Summary")
print(f"- **Learning Episodes:** {len(learning_rewards)}")
print(f"- **Testing Episodes:** {len(testing_rewards)}")
if learning_rewards:
    print(f"- **Learning Reward Range:** [{min(learning_rewards):.3f}, {max(learning_rewards):.3f}]")
    print(f"- **Learning Reward Mean:** {np.mean(learning_rewards):.3f} ± {np.std(learning_rewards):.3f}")
if testing_rewards:
    print(f"- **Testing Reward Range:** [{min(testing_rewards):.3f}, {max(testing_rewards):.3f}]")
    print(f"- **Testing Reward Mean:** {np.mean(testing_rewards):.3f} ± {np.std(testing_rewards):.3f}")
# %% [markdown]
# ## Learning Progress Visualization
#
# This section presents the learning progress with visualizations of rewards and steps.

# %%
# Create learning progress plots
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# Plot 1: Learning rewards over time
ax1 = axes[0, 0]
ax1.plot(learning_rewards, alpha=0.7, label='Episode Reward', color='blue')
ax1.set_title('Learning Progress - Rewards')
ax1.set_xlabel('Episode')
ax1.set_ylabel('Reward')
ax1.grid(True, alpha=0.3)

# Add moving average
window_size = min(50, len(learning_rewards) // 10)
if window_size > 1:
    moving_avg = pd.Series(learning_rewards).rolling(window=window_size).mean()
    ax1.plot(moving_avg, label=f'Moving Average (window={window_size})', linewidth=2, color='red')
ax1.legend()

# Plot 2: Learning steps over time
ax2 = axes[0, 1]
ax2.plot(learning_steps, alpha=0.7, label='Episode Steps', color='green')
ax2.set_title('Learning Progress - Steps')
ax2.set_xlabel('Episode')
ax2.set_ylabel('Steps')
ax2.grid(True, alpha=0.3)

if window_size > 1:
    moving_avg_steps = pd.Series(learning_steps).rolling(window=window_size).mean()
    ax2.plot(moving_avg_steps, label=f'Moving Average (window={window_size})', linewidth=2, color='red')
ax2.legend()

# Plot 3: Reward distribution
ax3 = axes[1, 0]
ax3.hist(learning_rewards, bins=30, alpha=0.7, edgecolor='black', color='blue', label='Reward Distribution')
ax3.set_title('Reward Distribution (Learning)')
ax3.set_xlabel('Reward')
ax3.set_ylabel('Frequency')
ax3.legend()
ax3.grid(True, alpha=0.3)

# Plot 4: Steps distribution
ax4 = axes[1, 1]
ax4.hist(learning_steps, bins=30, alpha=0.7, edgecolor='black', color='green', label='Steps Distribution')
ax4.set_title('Steps Distribution (Learning)')
ax4.set_xlabel('Steps')
ax4.set_ylabel('Frequency')
ax4.legend()
ax4.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# %% [markdown]
# ### Learning Statistics

# %%
# Learning statistics
print("### Learning Statistics")
print(f"- **Mean Reward:** {np.mean(learning_rewards):.3f} ± {np.std(learning_rewards):.3f}")
print(f"- **Mean Steps:** {np.mean(learning_steps):.3f} ± {np.std(learning_steps):.3f}")
print(f"- **Success Rate:** {(np.array(learning_rewards) > 0).mean() * 100:.1f}%")

if testing_rewards:
    # %% [markdown]
    # ## Final Model Evaluation
    #
    # This section presents the final evaluation of the model on test data.

    # %%
    # Create evaluation plots
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))

    # Boxplot for testing rewards
    ax1 = axes[0]
    ax1.boxplot(testing_rewards, labels=['Testing Rewards'])
    ax1.set_title('Final Model Evaluation - Rewards')
    ax1.set_ylabel('Reward')
    ax1.grid(True, alpha=0.3)

    # Boxplot for testing steps
    ax2 = axes[1]
    ax2.boxplot(testing_steps, labels=['Testing Steps'])
    ax2.set_title('Final Model Evaluation - Steps')
    ax2.set_ylabel('Steps')
    ax2.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

    # %% [markdown]
    # ### Evaluation Statistics

    # %%
    # Evaluation statistics
    print("### Evaluation Statistics")
    print(f"- **Mean Reward:** {np.mean(testing_rewards):.3f} ± {np.std(testing_rewards):.3f}")
    print(f"- **Mean Steps:** {np.mean(testing_steps):.3f} ± {np.std(testing_steps):.3f}")
    print(f"- **Success Rate:** {(np.array(testing_rewards) > 0).mean() * 100:.1f}%")
    print(f"- **Min Reward:** {np.min(testing_rewards):.3f}")
    print(f"- **Max Reward:** {np.max(testing_rewards):.3f}")

# %% [markdown]
# ## Performance Analysis
#
# Comparative analysis of learning and testing performance.

# %%
if learning_rewards and testing_rewards:
    # Compare learning vs testing performance
    learning_mean = np.mean(learning_rewards)
    testing_mean = np.mean(testing_rewards)
    improvement = ((testing_mean - learning_mean) / abs(learning_mean)) * 100 if learning_mean != 0 else 0

    print(f"- **Learning Performance:** {learning_mean:.3f} ± {np.std(learning_rewards):.3f}")
    print(f"- **Testing Performance:** {testing_mean:.3f} ± {np.std(testing_rewards):.3f}")
    print(f"- **Performance Change:** {improvement:+.1f}%")

# %% [markdown]
# ## Conclusion

# %%
if learning_rewards and testing_rewards:
    # Final conclusion analysis
    print("## Conclusion")
    print(f"The experiment completed {len(learning_rewards)} learning episodes")
    if testing_rewards:
        print(f"and {len(testing_rewards)} testing episodes.")
    if np.mean(testing_rewards) > np.mean(learning_rewards):
        print("The model shows good generalization with testing performance exceeding learning performance.")
    else:
        print("The model may be overfitting as testing performance is lower than learning performance.")
# %% [markdown]
# ---
# *Report generated automatically by Hercule*